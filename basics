SparkContext vs SparkSession

1. SparkSession was introduced in spark 2.0
2. Before sparkSession , we had to create separate context for other api's like sql,hive,streaming in sparkContext
         
         import SparkContext,SqlContext
         conf = new SparkConf().setAppName("my apps");
         sparkContext = new SparkContext(conf);
         sqlContext = new SqlContext(sparkContext);
 
 3. After sparkSession was introduced all the context(hive,sql,streaming) is encapsulated inside the sparkcontext itself.
       Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.1
      /_/

Using Python version 3.9.13 (main, Oct 13 2022 21:15:33)
Spark context Web UI available at http://001ca2df9408:4040
Spark context available as 'sc' (master = local[*], app id = local-1684831169243).
SparkSession available as 'spark'.

>>> spark.sql
<bound method SparkSession.sql of <pyspark.sql.session.SparkSession object at 0x7f34c0bffdc0>>

#create empty rdd 
RDD = spark.sparkContext.parallelize([])
RDD = spark.sparkContext.emptyRDD()

RDD.isempty()

>>> schema = StructType([StructField('fname',StringType(),True),StructField('Lname',StringType(),True)])
>>> edf = spark.createDataframe([],schema)
>>> edf.show()
+-----+-----+                                                                   
|fname|Lname|
+-----+-----+
+-----+-----+

>>> edf.limit(1).count()
0                                                                               
>>> edf.take(1)
[]

#without Schema
>>> edf = spark.createDataFrame([],StructType([]))
>>> edf.show()
++                                                                              
||
++
++

#using MapPartition

>>> data = [('James','Smith','M',3000),
...   ('Anna','Rose','F',4100),
...   ('Robert','Williams','M',6200), 
... ]
>>> 
>>> columns = ["firstname","lastname","gender","salary"]
>>> df = spark.createDataFrame(data=data, schema = columns)
>>> df.show()
+---------+--------+------+------+                                              
|firstname|lastname|gender|salary|
+---------+--------+------+------+
|    James|   Smith|     M|  3000|
|     Anna|    Rose|     F|  4100|
|   Robert|Williams|     M|  6200|
+---------+--------+------+------+

>>> def reformat(partitiondata):
...     for row in partitiondata:
...         yield [row.firstname + "," + row.lastname,row.salary*10/100]
... 
>>> df1 = df.rdd.mapPartitions(reformat).toDF(["name","bonus"])
>>> df1.show()                                                                  
+---------------+-----+                                                         
|           name|bonus|
+---------------+-----+
|    James,Smith|300.0|
|      Anna,Rose|410.0|
|Robert,Williams|620.0|
+---------------+-----+

#Create list of data to prepare data frame
>>> person_list = [("Berry","","Allen",1,"M"),("Oliver","Queen","",2,"M"),("Robert","","Williams",3,"M"),("Tony","","Stark",4,"F"),("Rajiv","Mary","Kumar",5,"F")]

from pyspark.sql.types import StructType,StringType,IntegerType,StructField;

schema = StructType([StructField("firstname",StringType(),True),StructField("middlename",StringType(),True),StructField("lastname",StringType(),True),
         StructField("id",IntegerType(),True),StructField("gender",StringType(),True)]);

>>> df = spark.createDataFrame(data=person_list,schema=schema);
>>> df.printSchema();
root
 |-- firstname: string (nullable = true)
 |-- middlename: string (nullable = true)
 |-- lastname: string (nullable = true)
 |-- id: integer (nullable = true)
 |-- gender: string (nullable = true)

>>> df.show()
+---------+----------+--------+---+------+                                      
|firstname|middlename|lastname| id|gender|
+---------+----------+--------+---+------+
|    Berry|          |   Allen|  1|     M|
|   Oliver|     Queen|        |  2|     M|
|   Robert|          |Williams|  3|     M|
|     Tony|          |   Stark|  4|     F|
|    Rajiv|      Mary|   Kumar|  5|     F|
+---------+----------+--------+---+------+

>>> dfcsv = spark.read.option("header",True).csv("/input-data/departments.csv");
>>> dfcsv.printSchema();
root
 |-- DEPARTMENT_ID: string (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: string (nullable = true)

>>> dfcsv.show();

>>> dfcsv2 = spark.read.option("header",True).option("inferSchema",True).csv("/input-data/departments.csv");
>>> dfcsv2.printSchema();
root
 |-- DEPARTMENT_ID: integer (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: integer (nullable = true)
 
 >>> dfcsv.show();
+-------------+--------------------+----------+-----------+
|DEPARTMENT_ID|     DEPARTMENT_NAME|MANAGER_ID|LOCATION_ID|
+-------------+--------------------+----------+-----------+
|           10|      Administration|       200|       1700|
|           20|           Marketing|       201|       1800|
|           30|          Purchasing|       114|       1700|
|           40|     Human Resources|       203|       2400|
|           50|            Shipping|       121|       1500|
|           60|                  IT|       103|       1400|
|           70|    Public Relations|       204|       2700|
|           80|               Sales|       145|       2500|
|           90|           Executive|       100|       1700|
|          100|             Finance|       108|       1700|
|          110|          Accounting|       205|       1700|
|          120|            Treasury|        - |       1700|
|          130|       Corporate Tax|        - |       1700|
|          140|  Control And Credit|        - |       1700|
|          150|Shareholder Services|        - |       1700|
|          160|            Benefits|        - |       1700|
|          170|       Manufacturing|        - |       1700|
|          180|        Construction|        - |       1700|
|          190|         Contracting|        - |       1700|
|          200|          Operations|        - |       1700|
+-------------+--------------------+----------+-----------+

>>> dfemp = spark.read.option("header",True).option("inferSchema",True).csv("/input-data/employees.csv");
>>> dfemp.printSchema();
root
 |-- EMPLOYEE_ID: integer (nullable = true)
 |-- FIRST_NAME: string (nullable = true)
 |-- LAST_NAME: string (nullable = true)
 |-- EMAIL: string (nullable = true)
 |-- PHONE_NUMBER: string (nullable = true)
 |-- HIRE_DATE: string (nullable = true)
 |-- JOB_ID: string (nullable = true)
 |-- SALARY: integer (nullable = true)
 |-- COMMISSION_PCT: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- DEPARTMENT_ID: integer (nullable = true)
 
 >>> df_tmp = dfemp.select("*").show();
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+
|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+
|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|
|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|
|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|
|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|
|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|
|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|
|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|
|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|
|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|
|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|
|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|
|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|
|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|            - |       102|           60|
|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|            - |       103|           60|
|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|            - |       103|           60|
|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|            - |       103|           60|
|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|            - |       103|           60|
|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|
|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|
|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+


>>> df_tmp = dfemp.select("EMPLOYEE_ID","FIRST_NAME").show()
+-----------+----------+
|EMPLOYEE_ID|FIRST_NAME|
+-----------+----------+
|        198|    Donald|
|        199|   Douglas|
|        200|  Jennifer|
|        201|   Michael|
|        202|       Pat|
|        203|     Susan|
|        204|   Hermann|
|        205|   Shelley|
|        206|   William|
|        100|    Steven|
|        101|     Neena|
|        102|       Lex|
|        103| Alexander|
|        104|     Bruce|
|        105|     David|
|        106|     Valli|
|        107|     Diana|
|        108|     Nancy|
|        109|    Daniel|
|        110|      John|
+-----------+----------+
only showing top 20 rows

>>> df_tmp = dfemp.select(dfemp["EMPLOYEE_ID"],dfemp["FIRST_NAME"]).show()
+-----------+----------+
|EMPLOYEE_ID|FIRST_NAME|
+-----------+----------+
|        198|    Donald|
|        199|   Douglas|
|        200|  Jennifer|
|        201|   Michael|
|        202|       Pat|
|        203|     Susan|
|        204|   Hermann|
|        205|   Shelley|
|        206|   William|
|        100|    Steven|
|        101|     Neena|
|        102|       Lex|
|        103| Alexander|
|        104|     Bruce|
|        105|     David|
|        106|     Valli|
|        107|     Diana|
|        108|     Nancy|
|        109|    Daniel|
|        110|      John|
+-----------+----------+
only showing top 20 rows

>>> from pyspark.sql.functions import col;
>>> df_tmp = dfemp.select(col("EMPLOYEE_ID"),col("FIRST_NAME")).show()
+-----------+----------+
|EMPLOYEE_ID|FIRST_NAME|
+-----------+----------+
|        198|    Donald|
|        199|   Douglas|
|        200|  Jennifer|
|        201|   Michael|
|        202|       Pat|
|        203|     Susan|
|        204|   Hermann|
|        205|   Shelley|
|        206|   William|
|        100|    Steven|
|        101|     Neena|
|        102|       Lex|
|        103| Alexander|
|        104|     Bruce|
|        105|     David|
|        106|     Valli|
|        107|     Diana|
|        108|     Nancy|
|        109|    Daniel|
|        110|      John|
+-----------+----------+
only showing top 20 rows


>>> df_tmp = dfemp.select(col("EMPLOYEE_ID").alias("EMP_ID"),col("FIRST_NAME").alias("F_NAME")).show()
+------+---------+
|EMP_ID|   F_NAME|
+------+---------+
|   198|   Donald|
|   199|  Douglas|
|   200| Jennifer|
|   201|  Michael|
|   202|      Pat|
|   203|    Susan|
|   204|  Hermann|
|   205|  Shelley|
|   206|  William|
|   100|   Steven|
|   101|    Neena|
|   102|      Lex|
|   103|Alexander|
|   104|    Bruce|
|   105|    David|
|   106|    Valli|
|   107|    Diana|
|   108|    Nancy|
|   109|   Daniel|
|   110|     John|
+------+---------+
only showing top 20 rows

>>> df_tmp = dfemp.withColumn("NEW_SALARY",col("SALARY")+5000).select(col("EMPLOYEE_ID"),col("SALARY"),col("NEW_SALARY")).show()
+-----------+------+----------+
|EMPLOYEE_ID|SALARY|NEW_SALARY|
+-----------+------+----------+
|        198|  2600|      7600|
|        199|  2600|      7600|
|        200|  4400|      9400|
|        201| 13000|     18000|
|        202|  6000|     11000|
|        203|  6500|     11500|
|        204| 10000|     15000|
|        205| 12008|     17008|
|        206|  8300|     13300|
|        100| 24000|     29000|
|        101| 17000|     22000|
|        102| 17000|     22000|
|        103|  9000|     14000|
|        104|  6000|     11000|
|        105|  4800|      9800|
|        106|  4800|      9800|
|        107|  4200|      9200|
|        108| 12008|     17008|
|        109|  9000|     14000|
|        110|  8200|     13200|
+-----------+------+----------+

>>> dftmp = dfemp.select(col("FIRST_NAME")).withColumn("NEW_SALARY",col("SALARY")).show();
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/spark/python/pyspark/sql/dataframe.py", line 2478, in withColumn
    return DataFrame(self._jdf.withColumn(colName, col._jc), self.sql_ctx)
  File "/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/usr/local/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: cannot resolve 'SALARY' given input columns: [FIRST_NAME];
'Project [FIRST_NAME#17, 'SALARY AS NEW_SALARY#95]
+- Project [FIRST_NAME#17]
   +- Relation [EMPLOYEE_ID#16,FIRST_NAME#17,LAST_NAME#18,EMAIL#19,PHONE_NUMBER#20,HIRE_DATE#21,JOB_ID#22,SALARY#23,COMMISSION_PCT#24,MANAGER_ID#25,DEPARTMENT_ID#26]
   
   >>> dfemp.withColumnRenamed("SALARY","NEW_SALARY").show();
+-----------+----------+---------+--------+------------+---------+----------+----------+--------------+----------+-------------+
|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|NEW_SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|
+-----------+----------+---------+--------+------------+---------+----------+----------+--------------+----------+-------------+
|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|      2600|            - |       124|           50|
|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|      2600|            - |       124|           50|
|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|      4400|            - |       101|           10|
|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN|     13000|            - |       100|           20|
|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|      6000|            - |       201|           20|
|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|      6500|            - |       101|           40|
|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP|     10000|            - |       101|           70|
|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR|     12008|            - |       101|          110|
|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|      8300|            - |       205|          110|
|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES|     24000|            - |        - |           90|
|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP|     17000|            - |       100|           90|
|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP|     17000|            - |       100|           90|
|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|      9000|            - |       102|           60|
|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|      6000|            - |       103|           60|
|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|      4800|            - |       103|           60|
|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|      4800|            - |       103|           60|
|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|      4200|            - |       103|           60|
|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR|     12008|            - |       101|          100|
|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|      9000|            - |       108|          100|
|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|      8200|            - |       108|          100|
+-----------+----------+---------+--------+------------+---------+----------+----------+--------------+----------+-------------+

>>> dfemp.drop("COMMISSION_PCT").show()
+-----------+----------+---------+--------+------------+---------+----------+------+----------+-------------+
|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|MANAGER_ID|DEPARTMENT_ID|
+-----------+----------+---------+--------+------------+---------+----------+------+----------+-------------+
|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|       124|           50|
|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|       124|           50|
|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|       101|           10|
|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|       100|           20|
|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|       201|           20|
|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|       101|           40|
|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|       101|           70|
|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|       101|          110|
|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|       205|          110|
|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|        - |           90|
|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|       100|           90|
|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|       100|           90|
|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|       102|           60|
|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|       103|           60|
|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|       103|           60|
|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|       103|           60|
|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|       103|           60|
|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|       101|          100|
|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|       108|          100|
|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|       108|          100|
+-----------+----------+---------+--------+------------+---------+----------+------+----------+-------------+

>>> dfemp.drop("COMMISSION_PCT").select(col("FIRST_NAME"),col("SALARY")).show()
+----------+------+
|FIRST_NAME|SALARY|
+----------+------+
|    Donald|  2600|
|   Douglas|  2600|
|  Jennifer|  4400|
|   Michael| 13000|
|       Pat|  6000|
|     Susan|  6500|
|   Hermann| 10000|
|   Shelley| 12008|
|   William|  8300|
|    Steven| 24000|
|     Neena| 17000|
|       Lex| 17000|
| Alexander|  9000|
|     Bruce|  6000|
|     David|  4800|
|     Valli|  4800|
|     Diana|  4200|
|     Nancy| 12008|
|    Daniel|  9000|
|      John|  8200|
+----------+------+
only showing top 20 rows

>>> dfemp.drop("COMMISSION_PCT").select(col("FIRST_NAME"),col("SALARY"),col("COMMISSION_PCT")).show()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/spark/python/pyspark/sql/dataframe.py", line 1685, in select
    jdf = self._jdf.select(self._jcols(*cols))
  File "/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/usr/local/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: cannot resolve 'COMMISSION_PCT' given input columns: [DEPARTMENT_ID, EMAIL, EMPLOYEE_ID, FIRST_NAME, HIRE_DATE, JOB_ID, LAST_NAME, MANAGER_ID, PHONE_NUMBER, SALARY];
'Project [FIRST_NAME#17, SALARY#23, 'COMMISSION_PCT]
+- Project [EMPLOYEE_ID#16, FIRST_NAME#17, LAST_NAME#18, EMAIL#19, PHONE_NUMBER#20, HIRE_DATE#21, JOB_ID#22, SALARY#23, MANAGER_ID#25, DEPARTMENT_ID#26]
   +- Relation [EMPLOYEE_ID#16,FIRST_NAME#17,LAST_NAME#18,EMAIL#19,PHONE_NUMBER#20,HIRE_DATE#21,JOB_ID#22,SALARY#23,COMMISSION_PCT#24,MANAGER_ID#25,DEPARTMENT_ID#26] csv
   
   
 >>> dfemp.filter(col("SALARY") > 5000).show();
+-----------+-----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+
|EMPLOYEE_ID| FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|
+-----------+-----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+
|        201|    Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|
|        202|        Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|
|        203|      Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|
|        204|    Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|
|        205|    Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|
|        206|    William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|
|        100|     Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|
|        101|      Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|
|        102|        Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|
|        103|  Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|            - |       102|           60|
|        104|      Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|            - |       103|           60|
|        108|      Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|
|        109|     Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|
|        110|       John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|
|        111|     Ismael|  Sciarra|ISCIARRA|515.124.4369|30-SEP-05|FI_ACCOUNT|  7700|            - |       108|          100|
|        112|Jose Manuel|    Urman| JMURMAN|515.124.4469|07-MAR-06|FI_ACCOUNT|  7800|            - |       108|          100|
|        113|       Luis|     Popp|   LPOPP|515.124.4567|07-DEC-07|FI_ACCOUNT|  6900|            - |       108|          100|
|        114|        Den| Raphaely|DRAPHEAL|515.127.4561|07-DEC-02|    PU_MAN| 11000|            - |       100|           30|
|        120|    Matthew|    Weiss|  MWEISS|650.123.1234|18-JUL-04|    ST_MAN|  8000|            - |       100|           50|
|        121|       Adam|    Fripp|  AFRIPP|650.123.2234|10-APR-05|    ST_MAN|  8200|            - |       100|           50|
+-----------+-----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+
only showing top 20 rows

>>> dfemp.filter(col("SALARY") > 5000).select(col("FIRST_NAME"),col("SALARY")).show();
+-----------+------+
| FIRST_NAME|SALARY|
+-----------+------+
|    Michael| 13000|
|        Pat|  6000|
|      Susan|  6500|
|    Hermann| 10000|
|    Shelley| 12008|
|    William|  8300|
|     Steven| 24000|
|      Neena| 17000|
|        Lex| 17000|
|  Alexander|  9000|
|      Bruce|  6000|
|      Nancy| 12008|
|     Daniel|  9000|
|       John|  8200|
|     Ismael|  7700|
|Jose Manuel|  7800|
|       Luis|  6900|
|        Den| 11000|
|    Matthew|  8000|
|       Adam|  8200|
+-----------+------+

>>> dfemp.filter((col("SALARY") > 5000) & (col("DEPARTMENT_ID") == 50)).select(col("FIRST_NAME"),col("DEPARTMENT_ID"),col("SALARY")).show();
+----------+-------------+------+
|FIRST_NAME|DEPARTMENT_ID|SALARY|
+----------+-------------+------+
|   Matthew|           50|  8000|
|      Adam|           50|  8200|
|     Payam|           50|  7900|
|    Shanta|           50|  6500|
|     Kevin|           50|  5800|
+----------+-------------+------+

>>> dfemp.filter(("SALARY > 5000 and DEPARTMENT_ID = 50")).select(col("FIRST_NAME"),col("DEPARTMENT_ID"),col("SALARY")).show();
+----------+-------------+------+
|FIRST_NAME|DEPARTMENT_ID|SALARY|
+----------+-------------+------+
|   Matthew|           50|  8000|
|      Adam|           50|  8200|
|     Payam|           50|  7900|
|    Shanta|           50|  6500|
|     Kevin|           50|  5800|
+----------+-------------+------+

dfemp.dropDuplicates().show() 
dfemp.dropDuplicates(["DEPARTMENT_ID","HIRE_DATE"]).show();
dfemp.distinct().show();

>>> dfemp.select(avg("SALARY").alias("avg_SALARY"),max("SALARY").alias("MAX_SALARY"),min("SALARY").alias("MIN_SALARY")).show();
+----------+----------+----------+
|avg_SALARY|MAX_SALARY|MIN_SALARY|
+----------+----------+----------+
|   6182.32|     24000|      2100|
+----------+----------+----------+

>>> dfemp.select(avg("SALARY").alias("avg_SALARY"),max("SALARY").alias("MAX_SALARY"),min("SALARY").alias("MIN_SALARY"),sum("SALARY")).show();
+----------+----------+----------+-----------+
|avg_SALARY|MAX_SALARY|MIN_SALARY|sum(SALARY)|
+----------+----------+----------+-----------+
|   6182.32|     24000|      2100|     309116|
+----------+----------+----------+-----------+

>>> dfemp.select(col("EMPLOYEE_ID"),max("SALARY").alias("MAX_SALARY")).show();
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/local/spark/python/pyspark/sql/dataframe.py", line 1685, in select
    jdf = self._jdf.select(self._jcols(*cols))
  File "/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/usr/local/spark/python/pyspark/sql/utils.py", line 117, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: grouping expressions sequence is empty, and 'EMPLOYEE_ID' is not an aggregate function. Wrap '(max(SALARY) AS MAX_SALARY)' in windowing function(s) or wrap 'EMPLOYEE_ID' in first() (or first_value) if you don't care which value you get.;
Aggregate [EMPLOYEE_ID#16, max(SALARY#23) AS MAX_SALARY#1215]
+- Relation [EMPLOYEE_ID#16,FIRST_NAME#17,LAST_NAME#18,EMAIL#19,PHONE_NUMBER#20,HIRE_DATE#21,JOB_ID#22,SALARY#23,COMMISSION_PCT#24,MANAGER_ID#25,DEPARTMENT_ID#26] csv

>>> dfemp.select("EMPLOYEE_ID","SALARY").orderBy("SALARY").show();

>>> dfemp.groupBy("DEPARTMENT_ID","JOB_ID").sum("SALARY").orderBy("DEPARTMENT_ID").show();
+-------------+----------+-----------+
|DEPARTMENT_ID|    JOB_ID|sum(SALARY)|
+-------------+----------+-----------+
|           10|   AD_ASST|       4400|
|           20|    MK_REP|       6000|
|           20|    MK_MAN|      13000|
|           30|  PU_CLERK|      13900|
|           30|    PU_MAN|      11000|
|           40|    HR_REP|       6500|
|           50|    ST_MAN|      36400|
|           50|  ST_CLERK|      44000|
|           50|  SH_CLERK|       5200|
|           60|   IT_PROG|      28800|
|           70|    PR_REP|      10000|
|           90|   AD_PRES|      24000|
|           90|     AD_VP|      34000|
|          100|    FI_MGR|      12008|
|          100|FI_ACCOUNT|      39600|
|          110|    AC_MGR|      12008|
|          110|AC_ACCOUNT|       8300|
+-------------+----------+-----------+

>>> dfemp.filter((col("DEPARTMENT_ID") == 110)).select(col("DEPARTMENT_ID"),col("JOB_ID"),col("SALARY")).show();
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'col' is not defined
>>> from pyspark.sql.functions import *
>>> dfemp.filter((col("DEPARTMENT_ID") == 110)).select(col("DEPARTMENT_ID"),col("JOB_ID"),col("SALARY")).show();
+-------------+----------+------+
|DEPARTMENT_ID|    JOB_ID|SALARY|
+-------------+----------+------+
|          110|    AC_MGR| 12008|
|          110|AC_ACCOUNT|  8300|
+-------------+----------+------+

>>> dfemp.groupBy("DEPARTMENT_ID","JOB_ID").sum("SALARY","EMPLOYEE_ID").orderBy("DEPARTMENT_ID").show();
+-------------+----------+-----------+----------------+
|DEPARTMENT_ID|    JOB_ID|sum(SALARY)|sum(EMPLOYEE_ID)|
+-------------+----------+-----------+----------------+
|           10|   AD_ASST|       4400|             200|
|           20|    MK_REP|       6000|             202|
|           20|    MK_MAN|      13000|             201|
|           30|  PU_CLERK|      13900|             585|
|           30|    PU_MAN|      11000|             114|
|           40|    HR_REP|       6500|             203|
|           50|    ST_MAN|      36400|             610|
|           50|  ST_CLERK|      44000|            2120|
|           50|  SH_CLERK|       5200|             397|
|           60|   IT_PROG|      28800|             525|
|           70|    PR_REP|      10000|             204|
|           90|   AD_PRES|      24000|             100|
|           90|     AD_VP|      34000|             203|
|          100|    FI_MGR|      12008|             108|
|          100|FI_ACCOUNT|      39600|             555|
|          110|    AC_MGR|      12008|             205|
|          110|AC_ACCOUNT|       8300|             206|
+-------------+----------+-----------+----------------+

>>> dfemp.filter((col("DEPARTMENT_ID") == 110)).select(col("DEPARTMENT_ID"),col("JOB_ID"),col("SALARY"),col("EMPLOYEE_ID")).show();
+-------------+----------+------+-----------+
|DEPARTMENT_ID|    JOB_ID|SALARY|EMPLOYEE_ID|
+-------------+----------+------+-----------+
|          110|    AC_MGR| 12008|        205|
|          110|AC_ACCOUNT|  8300|        206|
+-------------+----------+------+-----------+

>>> dfemp.groupBy("DEPARTMENT_ID","JOB_ID").agg(sum("SALARY").alias("sum salary"),max("SALARY").alias("max salary")).orderBy("DEPARTMENT_ID").show();
+-------------+----------+----------+----------+
|DEPARTMENT_ID|    JOB_ID|sum salary|max salary|
+-------------+----------+----------+----------+
|           10|   AD_ASST|      4400|      4400|
|           20|    MK_REP|      6000|      6000|
|           20|    MK_MAN|     13000|     13000|
|           30|  PU_CLERK|     13900|      3100|
|           30|    PU_MAN|     11000|     11000|
|           40|    HR_REP|      6500|      6500|
|           50|    ST_MAN|     36400|      8200|
|           50|  ST_CLERK|     44000|      3600|
|           50|  SH_CLERK|      5200|      2600|
|           60|   IT_PROG|     28800|      9000|
|           70|    PR_REP|     10000|     10000|
|           90|   AD_PRES|     24000|     24000|
|           90|     AD_VP|     34000|     17000|
|          100|    FI_MGR|     12008|     12008|
|          100|FI_ACCOUNT|     39600|      9000|
|          110|    AC_MGR|     12008|     12008|
|          110|AC_ACCOUNT|      8300|      8300|
+-------------+----------+----------+----------+

>>> dfemp.groupBy("DEPARTMENT_ID").agg(sum("SALARY").alias("sum salary"),max("SALARY").alias("max salary")).orderBy("DEPARTMENT_ID").show();
+-------------+----------+----------+
|DEPARTMENT_ID|sum salary|max salary|
+-------------+----------+----------+
|           10|      4400|      4400|
|           20|     19000|     13000|
|           30|     24900|     11000|
|           40|      6500|      6500|
|           50|     85600|      8200|
|           60|     28800|      9000|
|           70|     10000|     10000|
|           90|     58000|     24000|
|          100|     51608|     12008|
|          110|     20308|     12008|
+-------------+----------+----------+

dfemp.groupBy("DEPARTMENT_ID").agg(sum("SALARY").alias("sum salary"),max("SALARY").alias("max salary")).where(col("max salary")>10000).orderBy("DEPARTMENT_ID").show();
+-------------+----------+----------+
|DEPARTMENT_ID|sum salary|max salary|
+-------------+----------+----------+
|           20|     19000|     13000|
|           30|     24900|     11000|
|           90|     58000|     24000|
|          100|     51608|     12008|
|          110|     20308|     12008|
+-------------+----------+----------+

>>> df = dfemp.withColumn("Emp Grade",when(col("SALARY")>15000,"A").when((col("SALARY")>10000) & (col("SALARY")<15000),"B").otherwise("C"))
>>> df.show()
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+---------+
|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|Emp Grade|
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+---------+
|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|        C|
|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|        C|
|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|        C|
|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|        B|
|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|        C|
|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|        C|
|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|        C|
|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|        B|
|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|        C|
|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|        A|
|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|        A|
|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|        A|
|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|            - |       102|           60|        C|
|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|            - |       103|           60|        C|
|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|            - |       103|           60|        C|
|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|            - |       103|           60|        C|
|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|            - |       103|           60|        C|
|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|        B|
|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|        C|
|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|        C|
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+---------+

>>> df.select(col("EMPLOYEE_ID"),col("SALARY"),col("Emp Grade")).show()
+-----------+------+---------+
|EMPLOYEE_ID|SALARY|Emp Grade|
+-----------+------+---------+
|        198|  2600|        C|
|        199|  2600|        C|
|        200|  4400|        C|
|        201| 13000|        B|
|        202|  6000|        C|
|        203|  6500|        C|
|        204| 10000|        C|
|        205| 12008|        B|
|        206|  8300|        C|
|        100| 24000|        A|
|        101| 17000|        A|
|        102| 17000|        A|
|        103|  9000|        C|
|        104|  6000|        C|
|        105|  4800|        C|
|        106|  4800|        C|
|        107|  4200|        C|
|        108| 12008|        B|
|        109|  9000|        C|
|        110|  8200|        C|
+-----------+------+---------+


+-----------+------+
|EMPLOYEE_ID|SALARY|
+-----------+------+
|        132|  2100|
|        136|  2200|
|        128|  2200|
|        127|  2400|
|        135|  2400|
|        131|  2500|
|        119|  2500|
|        140|  2500|
|        198|  2600|
|        199|  2600|
|        118|  2600|
|        126|  2700|
|        139|  2700|
|        130|  2800|
|        117|  2800|
|        116|  2900|
|        134|  2900|
|        115|  3100|
|        125|  3200|
|        138|  3200|
+-----------+------+
only showing top 20 rows

>>> dfemp.select("EMPLOYEE_ID","DEPARTMENT_ID","SALARY").orderBy(col("DEPARTMENT_ID").asc(),col("SALARY").desc()).show();
+-----------+-------------+------+
|EMPLOYEE_ID|DEPARTMENT_ID|SALARY|
+-----------+-------------+------+
|        200|           10|  4400|
|        201|           20| 13000|
|        202|           20|  6000|
|        114|           30| 11000|
|        115|           30|  3100|
|        116|           30|  2900|
|        117|           30|  2800|
|        118|           30|  2600|
|        119|           30|  2500|
|        203|           40|  6500|
|        121|           50|  8200|
|        120|           50|  8000|
|        122|           50|  7900|
|        123|           50|  6500|
|        124|           50|  5800|
|        137|           50|  3600|
|        133|           50|  3300|
|        129|           50|  3300|
|        125|           50|  3200|
|        138|           50|  3200|
+-----------+-------------+------+


>>> df = spark.read.text('/input-data/about_us.txt')
>>> df.show()
+--------------------+
|               value|
+--------------------+
|This Tutorial is ...|
|                    |
|      Spark Overview|
|Apache Spark is a...|
|                    |
|         Downloading|
|Get Spark from th...|
|                    |
|If you’d like to ...|
|                    |
|Spark runs on bot...|
|                    |
|Spark runs on Jav...|
|                    |
|For Python 3.9, A...|
+--------------------+

>>> df1 = df.withColumn('word',explode(split(col('value'),' ')));
>>> df1.show()
+--------------------+----------+
|               value|      word|
+--------------------+----------+
|This Tutorial is ...|      This|
|This Tutorial is ...|  Tutorial|
|This Tutorial is ...|        is|
|This Tutorial is ...| exclusive|
|This Tutorial is ...|       for|
|This Tutorial is ...|       the|
|This Tutorial is ...|    people|
|This Tutorial is ...|       who|
|This Tutorial is ...|       are|
|This Tutorial is ...|interested|
|This Tutorial is ...|        in|
|This Tutorial is ...|    making|
|This Tutorial is ...|     their|
|This Tutorial is ...|      foot|
|This Tutorial is ...|    prints|
|This Tutorial is ...|        in|
|This Tutorial is ...|       one|
|This Tutorial is ...|        of|
|This Tutorial is ...|       the|
|This Tutorial is ...|       top|
+--------------------+----------+
only showing top 20 rows

>>> df2=df1.groupBy('word').agg(count('word'))
>>> df2.show()
+-----------------+-----------+
|             word|count(word)|
+-----------------+-----------+
|    installation.|          1|
|           niche,|          1|
|          include|          2|
|      Processing,|          1|
|    Compatibility|          1|
|       2.12/2.13,|          1|
|              API|          1|
|               If|          1|
|                —|          1|
|         8/11/17,|          1|
|    documentation|          1|
|            (e.g.|          1|
|    MS-PowerApps,|          1|
|             It’s|          1|
|        exclusive|          1|
|             Data|          2|
|code/applications|          1|
|          graphs.|          1|
|             3.7+|          1|
|             rich|          1|
+-----------------+-----------+
only showing top 20 rows

>>> rdd=spark.sparkContext.textFile('/input-data/about_us.txt');
>>> rdd
/input-data/about_us.txt MapPartitionsRDD[54] at textFile at NativeMethodAccessorImpl.java:0
>>> rdd.collect();
['This Tutorial is exclusive for the people who are interested in making their foot prints in one of the top niche, trending technology in IT world. The content posted here are suitable for all learning professionals, Students, Teacher,  Freshers etc. Here, we talk about BigData analytics, Azure Data Engineering, Spark Scala, Spark with Python (PySpark), Azure Functions, Machine Learning concepts and techniques, Natural Language Processing, MS-PowerApps, and Data Visualization tools.', '', 'Spark Overview', 'Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.', '', 'Downloading', 'Get Spark from the downloads page of the project website. This documentation is for Spark version 3.3.1. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version by augmenting Spark’s classpath. Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI.', '', 'If you’d like to build Spark from source, visit Building Spark.', '', 'Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java. This should include JVMs on x86_64 and ARM64. It’s easy to run locally on one machine — all you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.', '', 'Spark runs on Java 8/11/17, Scala 2.12/2.13, Python 3.7+ and R 3.5+. Java 8 prior to version 8u201 support is deprecated as of Spark 3.2.0. When using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for. For example, when using Scala 2.13, use Spark compiled for 2.13, and compile code/applications for Scala 2.13 as well.', '', 'For Python 3.9, Arrow optimization and pandas UDFs might not work due to the supported Python versions in Apache Arrow. Please refer to the latest Python Compatibility page. For Java 11, -Dio.netty.tryReflectionSetAccessible=true is required additionally for Apache Arrow library. This prevents java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.(long, int) not available when Apache Arrow uses Netty internally.']

>>> rdd1=rdd.flatMap(lambda line:line.split(' '))
>>> rdd1
PythonRDD[55] at RDD at PythonRDD.scala:53
>>> rdd1.collect();
['This', 'Tutorial', 'is', 'exclusive', 'for', 'the', 'people', 'who', 'are', 'interested', 'in', 'making', 'their', 'foot', 'prints', 'in', 'one', 'of', 'the', 'top', 'niche,', 'trending', 'technology', 'in', 'IT', 'world.', 'The', 'content', 'posted', 'here', 'are', 'suitable', 'for', 'all', 'learning', 'professionals,', 'Students,', 'Teacher,', '', 'Freshers', 'etc.', 'Here,', 'we', 'talk', 'about', 'BigData', 'analytics,', 'Azure', 'Data', 'Engineering,', 'Spark', 'Scala,', 'Spark', 'with', 'Python', '(PySpark),', 'Azure', 'Functions,', 'Machine', 'Learning', 'concepts', 'and', 'techniques,', 'Natural', 'Language', 'Processing,', 'MS-PowerApps,', 'and', 'Data', 'Visualization', 'tools.', '', 'Spark', 'Overview', 'Apache', 'Spark', 'is', 'a', 'unified', 'analytics', 'engine', 'for', 'large-scale', 'data', 'processing.', 'It', 'provides', 'high-level', 'APIs', 'in', 'Java,', 'Scala,', 'Python', 'and', 'R,', 'and', 'an', 'optimized', 'engine', 'that', 'supports', 'general', 'execution', 'graphs.', 'It', 'also', 'supports', 'a', 'rich', 'set', 'of', 'higher-level', 'tools', 'including', 'Spark', 'SQL', 'for', 'SQL', 'and', 'structured', 'data', 'processing,', 'pandas', 'API', 'on', 'Spark', 'for', 'pandas', 'workloads,', 'MLlib', 'for', 'machine', 'learning,', 'GraphX', 'for', 'graph', 'processing,', 'and', 'Structured', 'Streaming', 'for', 'incremental', 'computation', 'and', 'stream', 'processing.', '', 'Downloading', 'Get', 'Spark', 'from', 'the', 'downloads', 'page', 'of', 'the', 'project', 'website.', 'This', 'documentation', 'is', 'for', 'Spark', 'version', '3.3.1.', 'Spark', 'uses', 'Hadoop’s', 'client', 'libraries', 'for', 'HDFS', 'and', 'YARN.', 'Downloads', 'are', 'pre-packaged', 'for', 'a', 'handful', 'of', 'popular', 'Hadoop', 'versions.', 'Users', 'can', 'also', 'download', 'a', '“Hadoop', 'free”', 'binary', 'and', 'run', 'Spark', 'with', 'any', 'Hadoop', 'version', 'by', 'augmenting', 'Spark’s', 'classpath.', 'Scala', 'and', 'Java', 'users', 'can', 'include', 'Spark', 'in', 'their', 'projects', 'using', 'its', 'Maven', 'coordinates', 'and', 'Python', 'users', 'can', 'install', 'Spark', 'from', 'PyPI.', '', 'If', 'you’d', 'like', 'to', 'build', 'Spark', 'from', 'source,', 'visit', 'Building', 'Spark.', '', 'Spark', 'runs', 'on', 'both', 'Windows', 'and', 'UNIX-like', 'systems', '(e.g.', 'Linux,', 'Mac', 'OS),', 'and', 'it', 'should', 'run', 'on', 'any', 'platform', 'that', 'runs', 'a', 'supported', 'version', 'of', 'Java.', 'This', 'should', 'include', 'JVMs', 'on', 'x86_64', 'and', 'ARM64.', 'It’s', 'easy', 'to', 'run', 'locally', 'on', 'one', 'machine', '—', 'all', 'you', 'need', 'is', 'to', 'have', 'java', 'installed', 'on', 'your', 'system', 'PATH,', 'or', 'the', 'JAVA_HOME', 'environment', 'variable', 'pointing', 'to', 'a', 'Java', 'installation.', '', 'Spark', 'runs', 'on', 'Java', '8/11/17,', 'Scala', '2.12/2.13,', 'Python', '3.7+', 'and', 'R', '3.5+.', 'Java', '8', 'prior', 'to', 'version', '8u201', 'support', 'is', 'deprecated', 'as', 'of', 'Spark', '3.2.0.', 'When', 'using', 'the', 'Scala', 'API,', 'it', 'is', 'necessary', 'for', 'applications', 'to', 'use', 'the', 'same', 'version', 'of', 'Scala', 'that', 'Spark', 'was', 'compiled', 'for.', 'For', 'example,', 'when', 'using', 'Scala', '2.13,', 'use', 'Spark', 'compiled', 'for', '2.13,', 'and', 'compile', 'code/applications', 'for', 'Scala', '2.13', 'as', 'well.', '', 'For', 'Python', '3.9,', 'Arrow', 'optimization', 'and', 'pandas', 'UDFs', 'might', 'not', 'work', 'due', 'to', 'the', 'supported', 'Python', 'versions', 'in', 'Apache', 'Arrow.', 'Please', 'refer', 'to', 'the', 'latest', 'Python', 'Compatibility', 'page.', 'For', 'Java', '11,', '-Dio.netty.tryReflectionSetAccessible=true', 'is', 'required', 'additionally', 'for', 'Apache', 'Arrow', 'library.', 'This', 'prevents', 'java.lang.UnsupportedOperationException:', 'sun.misc.Unsafe', 'or', 'java.nio.DirectByteBuffer.(long,', 'int)', 'not', 'available', 'when', 'Apache', 'Arrow', 'uses', 'Netty', 'internally.']


>>> rdd2=rdd.map(lambda line:(line,1))
>>> rdd2.collect();
[('This Tutorial is exclusive for the people who are interested in making their foot prints in one of the top niche, trending technology in IT world. The content posted here are suitable for all learning professionals, Students, Teacher,  Freshers etc. Here, we talk about BigData analytics, Azure Data Engineering, Spark Scala, Spark with Python (PySpark), Azure Functions, Machine Learning concepts and techniques, Natural Language Processing, MS-PowerApps, and Data Visualization tools.', 1), ('', 1), ('Spark Overview', 1), ('Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.', 1), ('', 1), ('Downloading', 1), ('Get Spark from the downloads page of the project website. This documentation is for Spark version 3.3.1. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version by augmenting Spark’s classpath. Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI.', 1), ('', 1), ('If you’d like to build Spark from source, visit Building Spark.', 1), ('', 1), ('Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java. This should include JVMs on x86_64 and ARM64. It’s easy to run locally on one machine — all you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.', 1), ('', 1), ('Spark runs on Java 8/11/17, Scala 2.12/2.13, Python 3.7+ and R 3.5+. Java 8 prior to version 8u201 support is deprecated as of Spark 3.2.0. When using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for. For example, when using Scala 2.13, use Spark compiled for 2.13, and compile code/applications for Scala 2.13 as well.', 1), ('', 1), ('For Python 3.9, Arrow optimization and pandas UDFs might not work due to the supported Python versions in Apache Arrow. Please refer to the latest Python Compatibility page. For Java 11, -Dio.netty.tryReflectionSetAccessible=true is required additionally for Apache Arrow library. This prevents java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.(long, int) not available when Apache Arrow uses Netty internally.', 1)]


>>> rdd2=rdd1.map(lambda line:(line,1))
>>> rdd2.collect();
[('This', 1), ('Tutorial', 1), ('is', 1), ('exclusive', 1), ('for', 1), ('the', 1), ('people', 1), ('who', 1), ('are', 1), ('interested', 1), ('in', 1), ('making', 1), ('their', 1), ('foot', 1), ('prints', 1), ('in', 1), ('one', 1), ('of', 1), ('the', 1), ('top', 1), ('niche,', 1), ('trending', 1), ('technology', 1), ('in', 1), ('IT', 1), ('world.', 1), ('The', 1), ('content', 1), ('posted', 1), ('here', 1), ('are', 1), ('suitable', 1), ('for', 1), ('all', 1), ('learning', 1), ('professionals,', 1), ('Students,', 1), ('Teacher,', 1), ('', 1), ('Freshers', 1), ('etc.', 1), ('Here,', 1), ('we', 1), ('talk', 1), ('about', 1), ('BigData', 1), ('analytics,', 1), ('Azure', 1), ('Data', 1), ('Engineering,', 1), ('Spark', 1), ('Scala,', 1), ('Spark', 1), ('with', 1), ('Python', 1), ('(PySpark),', 1), ('Azure', 1), ('Functions,', 1), ('Machine', 1), ('Learning', 1), ('concepts', 1), ('and', 1), ('techniques,', 1), ('Natural', 1), ('Language', 1), ('Processing,', 1), ('MS-PowerApps,', 1), ('and', 1), ('Data', 1), ('Visualization', 1), ('tools.', 1), ('', 1), ('Spark', 1), ('Overview', 1), ('Apache', 1), ('Spark', 1), ('is', 1), ('a', 1), ('unified', 1), ('analytics', 1), ('engine', 1), ('for', 1), ('large-scale', 1), ('data', 1), ('processing.', 1), ('It', 1), ('provides', 1), ('high-level', 1), ('APIs', 1), ('in', 1), ('Java,', 1), ('Scala,', 1), ('Python', 1), ('and', 1), ('R,', 1), ('and', 1), ('an', 1), ('optimized', 1), ('engine', 1), ('that', 1), ('supports', 1), ('general', 1), ('execution', 1), ('graphs.', 1), ('It', 1), ('also', 1), ('supports', 1), ('a', 1), ('rich', 1), ('set', 1), ('of', 1), ('higher-level', 1), ('tools', 1), ('including', 1), ('Spark', 1), ('SQL', 1), ('for', 1), ('SQL', 1), ('and', 1), ('structured', 1), ('data', 1), ('processing,', 1), ('pandas', 1), ('API', 1), ('on', 1), ('Spark', 1), ('for', 1), ('pandas', 1), ('workloads,', 1), ('MLlib', 1), ('for', 1), ('machine', 1), ('learning,', 1), ('GraphX', 1), ('for', 1), ('graph', 1), ('processing,', 1), ('and', 1), ('Structured', 1), ('Streaming', 1), ('for', 1), ('incremental', 1), ('computation', 1), ('and', 1), ('stream', 1), ('processing.', 1), ('', 1), ('Downloading', 1), ('Get', 1), ('Spark', 1), ('from', 1), ('the', 1), ('downloads', 1), ('page', 1), ('of', 1), ('the', 1), ('project', 1), ('website.', 1), ('This', 1), ('documentation', 1), ('is', 1), ('for', 1), ('Spark', 1), ('version', 1), ('3.3.1.', 1), ('Spark', 1), ('uses', 1), ('Hadoop’s', 1), ('client', 1), ('libraries', 1), ('for', 1), ('HDFS', 1), ('and', 1), ('YARN.', 1), ('Downloads', 1), ('are', 1), ('pre-packaged', 1), ('for', 1), ('a', 1), ('handful', 1), ('of', 1), ('popular', 1), ('Hadoop', 1), ('versions.', 1), ('Users', 1), ('can', 1), ('also', 1), ('download', 1), ('a', 1), ('“Hadoop', 1), ('free”', 1), ('binary', 1), ('and', 1), ('run', 1), ('Spark', 1), ('with', 1), ('any', 1), ('Hadoop', 1), ('version', 1), ('by', 1), ('augmenting', 1), ('Spark’s', 1), ('classpath.', 1), ('Scala', 1), ('and', 1), ('Java', 1), ('users', 1), ('can', 1), ('include', 1), ('Spark', 1), ('in', 1), ('their', 1), ('projects', 1), ('using', 1), ('its', 1), ('Maven', 1), ('coordinates', 1), ('and', 1), ('Python', 1), ('users', 1), ('can', 1), ('install', 1), ('Spark', 1), ('from', 1), ('PyPI.', 1), ('', 1), ('If', 1), ('you’d', 1), ('like', 1), ('to', 1), ('build', 1), ('Spark', 1), ('from', 1), ('source,', 1), ('visit', 1), ('Building', 1), ('Spark.', 1), ('', 1), ('Spark', 1), ('runs', 1), ('on', 1), ('both', 1), ('Windows', 1), ('and', 1), ('UNIX-like', 1), ('systems', 1), ('(e.g.', 1), ('Linux,', 1), ('Mac', 1), ('OS),', 1), ('and', 1), ('it', 1), ('should', 1), ('run', 1), ('on', 1), ('any', 1), ('platform', 1), ('that', 1), ('runs', 1), ('a', 1), ('supported', 1), ('version', 1), ('of', 1), ('Java.', 1), ('This', 1), ('should', 1), ('include', 1), ('JVMs', 1), ('on', 1), ('x86_64', 1), ('and', 1), ('ARM64.', 1), ('It’s', 1), ('easy', 1), ('to', 1), ('run', 1), ('locally', 1), ('on', 1), ('one', 1), ('machine', 1), ('—', 1), ('all', 1), ('you', 1), ('need', 1), ('is', 1), ('to', 1), ('have', 1), ('java', 1), ('installed', 1), ('on', 1), ('your', 1), ('system', 1), ('PATH,', 1), ('or', 1), ('the', 1), ('JAVA_HOME', 1), ('environment', 1), ('variable', 1), ('pointing', 1), ('to', 1), ('a', 1), ('Java', 1), ('installation.', 1), ('', 1), ('Spark', 1), ('runs', 1), ('on', 1), ('Java', 1), ('8/11/17,', 1), ('Scala', 1), ('2.12/2.13,', 1), ('Python', 1), ('3.7+', 1), ('and', 1), ('R', 1), ('3.5+.', 1), ('Java', 1), ('8', 1), ('prior', 1), ('to', 1), ('version', 1), ('8u201', 1), ('support', 1), ('is', 1), ('deprecated', 1), ('as', 1), ('of', 1), ('Spark', 1), ('3.2.0.', 1), ('When', 1), ('using', 1), ('the', 1), ('Scala', 1), ('API,', 1), ('it', 1), ('is', 1), ('necessary', 1), ('for', 1), ('applications', 1), ('to', 1), ('use', 1), ('the', 1), ('same', 1), ('version', 1), ('of', 1), ('Scala', 1), ('that', 1), ('Spark', 1), ('was', 1), ('compiled', 1), ('for.', 1), ('For', 1), ('example,', 1), ('when', 1), ('using', 1), ('Scala', 1), ('2.13,', 1), ('use', 1), ('Spark', 1), ('compiled', 1), ('for', 1), ('2.13,', 1), ('and', 1), ('compile', 1), ('code/applications', 1), ('for', 1), ('Scala', 1), ('2.13', 1), ('as', 1), ('well.', 1), ('', 1), ('For', 1), ('Python', 1), ('3.9,', 1), ('Arrow', 1), ('optimization', 1), ('and', 1), ('pandas', 1), ('UDFs', 1), ('might', 1), ('not', 1), ('work', 1), ('due', 1), ('to', 1), ('the', 1), ('supported', 1), ('Python', 1), ('versions', 1), ('in', 1), ('Apache', 1), ('Arrow.', 1), ('Please', 1), ('refer', 1), ('to', 1), ('the', 1), ('latest', 1), ('Python', 1), ('Compatibility', 1), ('page.', 1), ('For', 1), ('Java', 1), ('11,', 1), ('-Dio.netty.tryReflectionSetAccessible=true', 1), ('is', 1), ('required', 1), ('additionally', 1), ('for', 1), ('Apache', 1), ('Arrow', 1), ('library.', 1), ('This', 1), ('prevents', 1), ('java.lang.UnsupportedOperationException:', 1), ('sun.misc.Unsafe', 1), ('or', 1), ('java.nio.DirectByteBuffer.(long,', 1), ('int)', 1), ('not', 1), ('available', 1), ('when', 1), ('Apache', 1), ('Arrow', 1), ('uses', 1), ('Netty', 1), ('internally.', 1)]


>>> rdd1=rdd.filter(lambda a:'in' in a)
>>> rdd1.collect()
['This Tutorial is exclusive for the people who are interested in making their foot prints in one of the top niche, trending technology in IT world. The content posted here are suitable for all learning professionals, Students, Teacher,  Freshers etc. Here, we talk about BigData analytics, Azure Data Engineering, Spark Scala, Spark with Python (PySpark), Azure Functions, Machine Learning concepts and techniques, Natural Language Processing, MS-PowerApps, and Data Visualization tools.', 'Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.', 'Downloading', 'Get Spark from the downloads page of the project website. This documentation is for Spark version 3.3.1. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version by augmenting Spark’s classpath. Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI.', 'If you’d like to build Spark from source, visit Building Spark.', 'Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java. This should include JVMs on x86_64 and ARM64. It’s easy to run locally on one machine — all you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.', 'Spark runs on Java 8/11/17, Scala 2.12/2.13, Python 3.7+ and R 3.5+. Java 8 prior to version 8u201 support is deprecated as of Spark 3.2.0. When using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for. For example, when using Scala 2.13, use Spark compiled for 2.13, and compile code/applications for Scala 2.13 as well.', 'For Python 3.9, Arrow optimization and pandas UDFs might not work due to the supported Python versions in Apache Arrow. Please refer to the latest Python Compatibility page. For Java 11, -Dio.netty.tryReflectionSetAccessible=true is required additionally for Apache Arrow library. This prevents java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.(long, int) not available when Apache Arrow uses Netty internally.']

>>> rdd1.count()
8

>>> rdd=spark.sparkContext.textFile('/input-data/about_us.txt');
>>> rdd1 = rdd.flatMap(lambda x:x.split(' '))
>>> rdd2 = rdd1.map(lambda x:(x,1))
>>> rdd3 = rdd2.reduceByKey(lambda x,y:x+y)
>>> rdd3.collect();
[('Tutorial', 1), ('is', 7), ('exclusive', 1), ('are', 3), ('interested', 1), ('in', 6), ('making', 1), ('of', 7), ('niche,', 1), ('trending', 1), ('technology', 1), ('The', 1), ('suitable', 1), ('learning', 1), ('professionals,', 1), ('Students,', 1), ('Teacher,', 1), ('', 7), ('Freshers', 1), ('etc.', 1), ('we', 1), ('analytics,', 1), ('Azure', 2), ('Engineering,', 1), ('Spark', 18), ('Scala,', 2), ('Python', 7), ('Machine', 1), ('concepts', 1), ('techniques,', 1), ('MS-PowerApps,', 1), ('Overview', 1), ('Apache', 4), ('unified', 1), ('analytics', 1), ('engine', 2), ('It', 2), ('provides', 1), ('high-level', 1), ('APIs', 1), ('Java,', 1), ('an', 1), ('optimized', 1), ('supports', 2), ('execution', 1), ('set', 1), ('tools', 1), ('SQL', 2), ('processing,', 2), ('pandas', 3), ('MLlib', 1), ('machine', 2), ('learning,', 1), ('GraphX', 1), ('graph', 1), ('Structured', 1), ('incremental', 1), ('computation', 1), ('Downloading', 1), ('Get', 1), ('page', 1), ('project', 1), ('documentation', 1), ('version', 5), ('uses', 2), ('Hadoop’s', 1), ('HDFS', 1), ('YARN.', 1), ('pre-packaged', 1), ('handful', 1), ('Users', 1), ('free”', 1), ('binary', 1), ('run', 3), ('augmenting', 1), ('Java', 5), ('include', 2), ('projects', 1), ('using', 3), ('Maven', 1), ('coordinates', 1), ('install', 1), ('like', 1), ('visit', 1), ('Building', 1), ('Spark.', 1), ('both', 1), ('Windows', 1), ('UNIX-like', 1), ('systems', 1), ('(e.g.', 1), ('OS),', 1), ('platform', 1), ('Java.', 1), ('x86_64', 1), ('ARM64.', 1), ('It’s', 1), ('locally', 1), ('have', 1), ('java', 1), ('installed', 1), ('PATH,', 1), ('JAVA_HOME', 1), ('variable', 1), ('pointing', 1), ('installation.', 1), ('8/11/17,', 1), ('2.12/2.13,', 1), ('3.7+', 1), ('R', 1), ('8', 1), ('8u201', 1), ('support', 1), ('as', 2), ('When', 1), ('use', 2), ('was', 1), ('compiled', 2), ('for.', 1), ('example,', 1), ('when', 2), ('2.13,', 2), ('compile', 1), ('2.13', 1), ('optimization', 1), ('UDFs', 1), ('work', 1), ('due', 1), ('versions', 1), ('Please', 1), ('refer', 1), ('latest', 1), ('Compatibility', 1), ('page.', 1), ('11,', 1), ('-Dio.netty.tryReflectionSetAccessible=true', 1), ('library.', 1), ('prevents', 1), ('java.nio.DirectByteBuffer.(long,', 1), ('This', 4), ('for', 15), ('the', 9), ('people', 1), ('who', 1), ('their', 2), ('foot', 1), ('prints', 1), ('one', 2), ('top', 1), ('IT', 1), ('world.', 1), ('content', 1), ('posted', 1), ('here', 1), ('all', 2), ('Here,', 1), ('talk', 1), ('about', 1), ('BigData', 1), ('Data', 2), ('with', 2), ('(PySpark),', 1), ('Functions,', 1), ('Learning', 1), ('and', 17), ('Natural', 1), ('Language', 1), ('Processing,', 1), ('Visualization', 1), ('tools.', 1), ('a', 6), ('large-scale', 1), ('data', 2), ('processing.', 2), ('R,', 1), ('that', 3), ('general', 1), ('graphs.', 1), ('also', 2), ('rich', 1), ('higher-level', 1), ('including', 1), ('structured', 1), ('API', 1), ('on', 7), ('workloads,', 1), ('Streaming', 1), ('stream', 1), ('from', 3), ('downloads', 1), ('website.', 1), ('3.3.1.', 1), ('client', 1), ('libraries', 1), ('Downloads', 1), ('popular', 1), ('Hadoop', 2), ('versions.', 1), ('can', 3), ('download', 1), ('“Hadoop', 1), ('any', 2), ('by', 1), ('Spark’s', 1), ('classpath.', 1), ('Scala', 6), ('users', 2), ('its', 1), ('PyPI.', 1), ('If', 1), ('you’d', 1), ('to', 8), ('build', 1), ('source,', 1), ('runs', 3), ('Linux,', 1), ('Mac', 1), ('it', 2), ('should', 2), ('supported', 2), ('JVMs', 1), ('easy', 1), ('—', 1), ('you', 1), ('need', 1), ('your', 1), ('system', 1), ('or', 2), ('environment', 1), ('3.5+.', 1), ('prior', 1), ('deprecated', 1), ('3.2.0.', 1), ('API,', 1), ('necessary', 1), ('applications', 1), ('same', 1), ('For', 3), ('code/applications', 1), ('well.', 1), ('3.9,', 1), ('Arrow', 3), ('might', 1), ('not', 2), ('Arrow.', 1), ('required', 1), ('additionally', 1), ('java.lang.UnsupportedOperationException:', 1), ('sun.misc.Unsafe', 1), ('int)', 1), ('available', 1), ('Netty', 1), ('internally.', 1)]

>>> rdd=spark.sparkContext.textFile('/input-data/about_us.txt');
>>> rdd1=rdd.filter(lambda x:'in' in x)
>>> rdd1.collect();
['This Tutorial is exclusive for the people who are interested in making their foot prints in one of the top niche, trending technology in IT world. The content posted here are suitable for all learning professionals, Students, Teacher,  Freshers etc. Here, we talk about BigData analytics, Azure Data Engineering, Spark Scala, Spark with Python (PySpark), Azure Functions, Machine Learning concepts and techniques, Natural Language Processing, MS-PowerApps, and Data Visualization tools.', 'Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.', 'Downloading', 'Get Spark from the downloads page of the project website. This documentation is for Spark version 3.3.1. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version by augmenting Spark’s classpath. Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI.', 'If you’d like to build Spark from source, visit Building Spark.', 'Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java. This should include JVMs on x86_64 and ARM64. It’s easy to run locally on one machine — all you need is to have java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation.', 'Spark runs on Java 8/11/17, Scala 2.12/2.13, Python 3.7+ and R 3.5+. Java 8 prior to version 8u201 support is deprecated as of Spark 3.2.0. When using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for. For example, when using Scala 2.13, use Spark compiled for 2.13, and compile code/applications for Scala 2.13 as well.', 'For Python 3.9, Arrow optimization and pandas UDFs might not work due to the supported Python versions in Apache Arrow. Please refer to the latest Python Compatibility page. For Java 11, -Dio.netty.tryReflectionSetAccessible=true is required additionally for Apache Arrow library. This prevents java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.(long, int) not available when Apache Arrow uses Netty internally.']
>>> rdd2=rdd1.flatMap(lambda x:x.split(' '))
>>> rdd3=rdd2.map(lambda x:(x,1))
>>> rdd4=rdd3.reduceByKey(lambda x,y:x+y)
>>> rdd4.collect()
[('Tutorial', 1), ('is', 7), ('exclusive', 1), ('are', 3), ('interested', 1), ('in', 6), ('making', 1), ('of', 7), ('niche,', 1), ('trending', 1), ('technology', 1), ('The', 1), ('suitable', 1), ('learning', 1), ('professionals,', 1), ('Students,', 1), ('Teacher,', 1), ('', 1), ('Freshers', 1), ('etc.', 1), ('we', 1), ('analytics,', 1), ('Azure', 2), ('Engineering,', 1), ('Spark', 17), ('Scala,', 2), ('Python', 7), ('Machine', 1), ('concepts', 1), ('techniques,', 1), ('MS-PowerApps,', 1), ('Apache', 4), ('unified', 1), ('analytics', 1), ('engine', 2), ('It', 2), ('provides', 1), ('high-level', 1), ('APIs', 1), ('Java,', 1), ('an', 1), ('optimized', 1), ('supports', 2), ('execution', 1), ('set', 1), ('tools', 1), ('SQL', 2), ('processing,', 2), ('pandas', 3), ('MLlib', 1), ('machine', 2), ('learning,', 1), ('GraphX', 1), ('graph', 1), ('Structured', 1), ('incremental', 1), ('computation', 1), ('Downloading', 1), ('Get', 1), ('page', 1), ('project', 1), ('documentation', 1), ('version', 5), ('uses', 2), ('Hadoop’s', 1), ('HDFS', 1), ('YARN.', 1), ('pre-packaged', 1), ('handful', 1), ('Users', 1), ('free”', 1), ('binary', 1), ('run', 3), ('augmenting', 1), ('Java', 5), ('include', 2), ('projects', 1), ('using', 3), ('Maven', 1), ('coordinates', 1), ('install', 1), ('like', 1), ('visit', 1), ('Building', 1), ('Spark.', 1), ('both', 1), ('Windows', 1), ('UNIX-like', 1), ('systems', 1), ('(e.g.', 1), ('OS),', 1), ('platform', 1), ('Java.', 1), ('x86_64', 1), ('ARM64.', 1), ('It’s', 1), ('locally', 1), ('have', 1), ('java', 1), ('installed', 1), ('PATH,', 1), ('JAVA_HOME', 1), ('variable', 1), ('pointing', 1), ('installation.', 1), ('8/11/17,', 1), ('2.12/2.13,', 1), ('3.7+', 1), ('R', 1), ('8', 1), ('8u201', 1), ('support', 1), ('as', 2), ('When', 1), ('use', 2), ('was', 1), ('compiled', 2), ('for.', 1), ('example,', 1), ('when', 2), ('2.13,', 2), ('compile', 1), ('2.13', 1), ('optimization', 1), ('UDFs', 1), ('work', 1), ('due', 1), ('versions', 1), ('Please', 1), ('refer', 1), ('latest', 1), ('Compatibility', 1), ('page.', 1), ('11,', 1), ('-Dio.netty.tryReflectionSetAccessible=true', 1), ('library.', 1), ('prevents', 1), ('java.nio.DirectByteBuffer.(long,', 1), ('This', 4), ('for', 15), ('the', 9), ('people', 1), ('who', 1), ('their', 2), ('foot', 1), ('prints', 1), ('one', 2), ('top', 1), ('IT', 1), ('world.', 1), ('content', 1), ('posted', 1), ('here', 1), ('all', 2), ('Here,', 1), ('talk', 1), ('about', 1), ('BigData', 1), ('Data', 2), ('with', 2), ('(PySpark),', 1), ('Functions,', 1), ('Learning', 1), ('and', 17), ('Natural', 1), ('Language', 1), ('Processing,', 1), ('Visualization', 1), ('tools.', 1), ('a', 6), ('large-scale', 1), ('data', 2), ('processing.', 2), ('R,', 1), ('that', 3), ('general', 1), ('graphs.', 1), ('also', 2), ('rich', 1), ('higher-level', 1), ('including', 1), ('structured', 1), ('API', 1), ('on', 7), ('workloads,', 1), ('Streaming', 1), ('stream', 1), ('from', 3), ('downloads', 1), ('website.', 1), ('3.3.1.', 1), ('client', 1), ('libraries', 1), ('Downloads', 1), ('popular', 1), ('Hadoop', 2), ('versions.', 1), ('can', 3), ('download', 1), ('“Hadoop', 1), ('any', 2), ('by', 1), ('Spark’s', 1), ('classpath.', 1), ('Scala', 6), ('users', 2), ('its', 1), ('PyPI.', 1), ('If', 1), ('you’d', 1), ('to', 8), ('build', 1), ('source,', 1), ('runs', 3), ('Linux,', 1), ('Mac', 1), ('it', 2), ('should', 2), ('supported', 2), ('JVMs', 1), ('easy', 1), ('—', 1), ('you', 1), ('need', 1), ('your', 1), ('system', 1), ('or', 2), ('environment', 1), ('3.5+.', 1), ('prior', 1), ('deprecated', 1), ('3.2.0.', 1), ('API,', 1), ('necessary', 1), ('applications', 1), ('same', 1), ('For', 3), ('code/applications', 1), ('well.', 1), ('3.9,', 1), ('Arrow', 3), ('might', 1), ('not', 2), ('Arrow.', 1), ('required', 1), ('additionally', 1), ('java.lang.UnsupportedOperationException:', 1), ('sun.misc.Unsafe', 1), ('int)', 1), ('available', 1), ('Netty', 1), ('internally.', 1)]
>>> rdd5=rdd4.filter(lambda x:'in' in x)
>>> rdd5.collect()
[('in', 6)]

>>> df = spark.read.option('delimiter','.').text('/input-data/dude.txt')
>>> df
DataFrame[value: string]
>>> df.count()
5
>>> df.show()
+--------------------+
|               value|
+--------------------+
|hi dude how are you.|
|i am in the pool ...|
|pull me in your f...|
|          drag me to|
|          the world.|
+--------------------+

>>> rdd = spark.sparkContext.textFile('/input-data/dude.txt')
>>> rdd.count()
5
>>> rdd1 = rdd.filter(lambda x:'in' in x)
>>> rdd1.count()
2
>>> df.show()
+--------------------+
|               value|
+--------------------+
|hi dude how are you.|
|i am in the pool ...|
|pull me in your f...|
|          drag me to|
|          the world.|
+--------------------+

>>> df.filter(col('value')=='in').show()
+-----+
|value|
+-----+
+-----+

>>> df.filter(col('value')=='the world.').show()
+----------+
|     value|
+----------+
|the world.|
+----------+



>>> df.select(col('value').contains('in')).show()
+-------------------+
|contains(value, in)|
+-------------------+
|              false|
|               true|
|               true|
|              false|
|              false|
+-------------------+

>>> df.select(col('value').contains('in')).count()
5
>>> df.filter(col('value').contains('in')).count()
2

dfwhen.createOrReplaceTempView("emp_grade")

>>> dfsql = spark.sql("select * from emp_grade")
>>> dfsql.show()
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+---------+
|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|Emp Grade|
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+---------+
|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|        C|
|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|        C|
|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|        C|
|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|        B|
|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|        C|
|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|        C|
|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|        C|
|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|        B|
|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|        C|
|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|        A|
|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|        A|
|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|        A|
|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|            - |       102|           60|        C|
|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|            - |       103|           60|        C|
|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|            - |       103|           60|        C|
|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|            - |       103|           60|        C|
|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|            - |       103|           60|        C|
|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|        B|
|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|        C|
|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|        C|
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+---------+

>>> dfn = dfemp.withColumn("HIRE_NEW_DATE",unix_timestamp(col("HIRE_DATE"), "dd-MMM-yy").cast("timestamp")).select(col("EMPLOYEE_ID"),col("FIRST_NAME"),col("HIRE_DATE"),col("HIRE_NEW_DATE"),col("JOB_ID"),col("SALARY"),col("DEPARTMENT_ID"))
>>> dfn
DataFrame[EMPLOYEE_ID: int, FIRST_NAME: string, HIRE_DATE: string, HIRE_NEW_DATE: timestamp, JOB_ID: string, SALARY: int, DEPARTMENT_ID: int]
>>> dfn.show()
+-----------+----------+---------+-------------------+----------+------+-------------+
|EMPLOYEE_ID|FIRST_NAME|HIRE_DATE|      HIRE_NEW_DATE|    JOB_ID|SALARY|DEPARTMENT_ID|
+-----------+----------+---------+-------------------+----------+------+-------------+
|        198|    Donald|21-JUN-07|2007-06-21 00:00:00|  SH_CLERK|  2600|           50|
|        199|   Douglas|13-JAN-08|2008-01-13 00:00:00|  SH_CLERK|  2600|           50|
|        200|  Jennifer|17-SEP-03|2003-09-17 00:00:00|   AD_ASST|  4400|           10|
|        201|   Michael|17-FEB-04|2004-02-17 00:00:00|    MK_MAN| 13000|           20|
|        202|       Pat|17-AUG-05|2005-08-17 00:00:00|    MK_REP|  6000|           20|
|        203|     Susan|07-JUN-02|2002-06-07 00:00:00|    HR_REP|  6500|           40|
|        204|   Hermann|07-JUN-02|2002-06-07 00:00:00|    PR_REP| 10000|           70|
|        205|   Shelley|07-JUN-02|2002-06-07 00:00:00|    AC_MGR| 12008|          110|
|        206|   William|07-JUN-02|2002-06-07 00:00:00|AC_ACCOUNT|  8300|          110|
|        100|    Steven|17-JUN-03|2003-06-17 00:00:00|   AD_PRES| 24000|           90|
|        101|     Neena|21-SEP-05|2005-09-21 00:00:00|     AD_VP| 17000|           90|
|        102|       Lex|13-JAN-01|2001-01-13 00:00:00|     AD_VP| 17000|           90|
|        103| Alexander|03-JAN-06|2006-01-03 00:00:00|   IT_PROG|  9000|           60|
|        104|     Bruce|21-MAY-07|2007-05-21 00:00:00|   IT_PROG|  6000|           60|
|        105|     David|25-JUN-05|2005-06-25 00:00:00|   IT_PROG|  4800|           60|
|        106|     Valli|05-FEB-06|2006-02-05 00:00:00|   IT_PROG|  4800|           60|
|        107|     Diana|07-FEB-07|2007-02-07 00:00:00|   IT_PROG|  4200|           60|
|        108|     Nancy|17-AUG-02|2002-08-17 00:00:00|    FI_MGR| 12008|          100|
|        109|    Daniel|16-AUG-02|2002-08-16 00:00:00|FI_ACCOUNT|  9000|          100|
|        110|      John|28-SEP-05|2005-09-28 00:00:00|FI_ACCOUNT|  8200|          100|
+-----------+----------+---------+-------------------+----------+------+-------------+,


dfmon = dfn.withColumn("HIRE_MON_DATE",unix_timestamp(col("HIRE_DATE"), "dd-MMM-yy").cast("timestamp")).select(col("EMPLOYEE_ID"),col("FIRST_NAME"),col("HIRE_DATE"),col("HIRE_NEW_DATE"),col("JOB_ID"),col("SALARY"),col("DEPARTMENT_ID"),col("HIRE_MON_DATE"))



dfmon = dfn.withColumn("HIRE_MON_DATE",unix_timestamp(col("HIRE_DATE"), "dd-MMM-yy").cast("DateType()")).select(col("EMPLOYEE_ID"),col("FIRST_NAME"),col("HIRE_DATE"),col("HIRE_NEW_DATE"),col("JOB_ID"),col("SALARY"),col("DEPARTMENT_ID"),col("HIRE_MON_DATE")).show()

>>> dfmon.withColumn("mon",month(col("HIRE_MON_DATE"))).show()
+-----------+----------+---------+-------------------+----------+------+-------------+-------------------+---+
|EMPLOYEE_ID|FIRST_NAME|HIRE_DATE|      HIRE_NEW_DATE|    JOB_ID|SALARY|DEPARTMENT_ID|      HIRE_MON_DATE|mon|
+-----------+----------+---------+-------------------+----------+------+-------------+-------------------+---+
|        198|    Donald|21-JUN-07|2007-06-21 00:00:00|  SH_CLERK|  2600|           50|2007-06-21 00:00:00|  6|
|        199|   Douglas|13-JAN-08|2008-01-13 00:00:00|  SH_CLERK|  2600|           50|2008-01-13 00:00:00|  1|
|        200|  Jennifer|17-SEP-03|2003-09-17 00:00:00|   AD_ASST|  4400|           10|2003-09-17 00:00:00|  9|
|        201|   Michael|17-FEB-04|2004-02-17 00:00:00|    MK_MAN| 13000|           20|2004-02-17 00:00:00|  2|
|        202|       Pat|17-AUG-05|2005-08-17 00:00:00|    MK_REP|  6000|           20|2005-08-17 00:00:00|  8|
|        203|     Susan|07-JUN-02|2002-06-07 00:00:00|    HR_REP|  6500|           40|2002-06-07 00:00:00|  6|
|        204|   Hermann|07-JUN-02|2002-06-07 00:00:00|    PR_REP| 10000|           70|2002-06-07 00:00:00|  6|
|        205|   Shelley|07-JUN-02|2002-06-07 00:00:00|    AC_MGR| 12008|          110|2002-06-07 00:00:00|  6|
|        206|   William|07-JUN-02|2002-06-07 00:00:00|AC_ACCOUNT|  8300|          110|2002-06-07 00:00:00|  6|
|        100|    Steven|17-JUN-03|2003-06-17 00:00:00|   AD_PRES| 24000|           90|2003-06-17 00:00:00|  6|
|        101|     Neena|21-SEP-05|2005-09-21 00:00:00|     AD_VP| 17000|           90|2005-09-21 00:00:00|  9|
|        102|       Lex|13-JAN-01|2001-01-13 00:00:00|     AD_VP| 17000|           90|2001-01-13 00:00:00|  1|
|        103| Alexander|03-JAN-06|2006-01-03 00:00:00|   IT_PROG|  9000|           60|2006-01-03 00:00:00|  1|
|        104|     Bruce|21-MAY-07|2007-05-21 00:00:00|   IT_PROG|  6000|           60|2007-05-21 00:00:00|  5|
|        105|     David|25-JUN-05|2005-06-25 00:00:00|   IT_PROG|  4800|           60|2005-06-25 00:00:00|  6|
|        106|     Valli|05-FEB-06|2006-02-05 00:00:00|   IT_PROG|  4800|           60|2006-02-05 00:00:00|  2|
|        107|     Diana|07-FEB-07|2007-02-07 00:00:00|   IT_PROG|  4200|           60|2007-02-07 00:00:00|  2|
|        108|     Nancy|17-AUG-02|2002-08-17 00:00:00|    FI_MGR| 12008|          100|2002-08-17 00:00:00|  8|
|        109|    Daniel|16-AUG-02|2002-08-16 00:00:00|FI_ACCOUNT|  9000|          100|2002-08-16 00:00:00|  8|
|        110|      John|28-SEP-05|2005-09-28 00:00:00|FI_ACCOUNT|  8200|          100|2005-09-28 00:00:00|  9|
+-----------+----------+---------+-------------------+----------+------+-------------+-------------------+---+

>>> dfmon = dfn.withColumn("HIRE_MON_DATE",unix_timestamp(col("HIRE_DATE"), "dd-MMM-yy").cast("timestamp"))
>>> dfmon = dfn.withColumn("HIRE_MON_DATE",unix_timestamp(col("HIRE_DATE"), "dd-MMM-yy").cast("timestamp")).withColumn("Month",month(col("HIRE_MON_DATE")))
>>> dfmon = dfn.withColumn("HIRE_MON_DATE",unix_timestamp(col("HIRE_DATE"), "dd-MMM-yy").cast("timestamp")).withColumn("Month_Hired",month(col("HIRE_MON_DATE"))).withColumn("Year_Hired",col("HIRE_MON_DATE"))
>>> dfmon.show()
+-----------+----------+---------+-------------------+----------+------+-------------+-------------------+-----------+-------------------+
|EMPLOYEE_ID|FIRST_NAME|HIRE_DATE|      HIRE_NEW_DATE|    JOB_ID|SALARY|DEPARTMENT_ID|      HIRE_MON_DATE|Month_Hired|         Year_Hired|
+-----------+----------+---------+-------------------+----------+------+-------------+-------------------+-----------+-------------------+
|        198|    Donald|21-JUN-07|2007-06-21 00:00:00|  SH_CLERK|  2600|           50|2007-06-21 00:00:00|          6|2007-06-21 00:00:00|
|        199|   Douglas|13-JAN-08|2008-01-13 00:00:00|  SH_CLERK|  2600|           50|2008-01-13 00:00:00|          1|2008-01-13 00:00:00|
|        200|  Jennifer|17-SEP-03|2003-09-17 00:00:00|   AD_ASST|  4400|           10|2003-09-17 00:00:00|          9|2003-09-17 00:00:00|
|        201|   Michael|17-FEB-04|2004-02-17 00:00:00|    MK_MAN| 13000|           20|2004-02-17 00:00:00|          2|2004-02-17 00:00:00|
|        202|       Pat|17-AUG-05|2005-08-17 00:00:00|    MK_REP|  6000|           20|2005-08-17 00:00:00|          8|2005-08-17 00:00:00|
|        203|     Susan|07-JUN-02|2002-06-07 00:00:00|    HR_REP|  6500|           40|2002-06-07 00:00:00|          6|2002-06-07 00:00:00|
|        204|   Hermann|07-JUN-02|2002-06-07 00:00:00|    PR_REP| 10000|           70|2002-06-07 00:00:00|          6|2002-06-07 00:00:00|
|        205|   Shelley|07-JUN-02|2002-06-07 00:00:00|    AC_MGR| 12008|          110|2002-06-07 00:00:00|          6|2002-06-07 00:00:00|
|        206|   William|07-JUN-02|2002-06-07 00:00:00|AC_ACCOUNT|  8300|          110|2002-06-07 00:00:00|          6|2002-06-07 00:00:00|
|        100|    Steven|17-JUN-03|2003-06-17 00:00:00|   AD_PRES| 24000|           90|2003-06-17 00:00:00|          6|2003-06-17 00:00:00|
|        101|     Neena|21-SEP-05|2005-09-21 00:00:00|     AD_VP| 17000|           90|2005-09-21 00:00:00|          9|2005-09-21 00:00:00|
|        102|       Lex|13-JAN-01|2001-01-13 00:00:00|     AD_VP| 17000|           90|2001-01-13 00:00:00|          1|2001-01-13 00:00:00|
|        103| Alexander|03-JAN-06|2006-01-03 00:00:00|   IT_PROG|  9000|           60|2006-01-03 00:00:00|          1|2006-01-03 00:00:00|
|        104|     Bruce|21-MAY-07|2007-05-21 00:00:00|   IT_PROG|  6000|           60|2007-05-21 00:00:00|          5|2007-05-21 00:00:00|
|        105|     David|25-JUN-05|2005-06-25 00:00:00|   IT_PROG|  4800|           60|2005-06-25 00:00:00|          6|2005-06-25 00:00:00|
|        106|     Valli|05-FEB-06|2006-02-05 00:00:00|   IT_PROG|  4800|           60|2006-02-05 00:00:00|          2|2006-02-05 00:00:00|
|        107|     Diana|07-FEB-07|2007-02-07 00:00:00|   IT_PROG|  4200|           60|2007-02-07 00:00:00|          2|2007-02-07 00:00:00|
|        108|     Nancy|17-AUG-02|2002-08-17 00:00:00|    FI_MGR| 12008|          100|2002-08-17 00:00:00|          8|2002-08-17 00:00:00|
|        109|    Daniel|16-AUG-02|2002-08-16 00:00:00|FI_ACCOUNT|  9000|          100|2002-08-16 00:00:00|          8|2002-08-16 00:00:00|
|        110|      John|28-SEP-05|2005-09-28 00:00:00|FI_ACCOUNT|  8200|          100|2005-09-28 00:00:00|          9|2005-09-28 00:00:00|
+-----------+----------+---------+-------------------+----------+------+-------------+-------------------+-----------+-------------------+
only showing top 20 rows

>>> dfmon = dfn.withColumn("HIRE_MON_DATE",unix_timestamp(col("HIRE_DATE"), "dd-MMM-yy").cast("timestamp")).withColumn("Month_Hired",month(col("HIRE_MON_DATE"))).withColumn("Year_Hired",year(col("HIRE_MON_DATE")))
>>> dfmon.show()
+-----------+----------+---------+-------------------+----------+------+-------------+-------------------+-----------+----------+
|EMPLOYEE_ID|FIRST_NAME|HIRE_DATE|      HIRE_NEW_DATE|    JOB_ID|SALARY|DEPARTMENT_ID|      HIRE_MON_DATE|Month_Hired|Year_Hired|
+-----------+----------+---------+-------------------+----------+------+-------------+-------------------+-----------+----------+
|        198|    Donald|21-JUN-07|2007-06-21 00:00:00|  SH_CLERK|  2600|           50|2007-06-21 00:00:00|          6|      2007|
|        199|   Douglas|13-JAN-08|2008-01-13 00:00:00|  SH_CLERK|  2600|           50|2008-01-13 00:00:00|          1|      2008|
|        200|  Jennifer|17-SEP-03|2003-09-17 00:00:00|   AD_ASST|  4400|           10|2003-09-17 00:00:00|          9|      2003|
|        201|   Michael|17-FEB-04|2004-02-17 00:00:00|    MK_MAN| 13000|           20|2004-02-17 00:00:00|          2|      2004|
|        202|       Pat|17-AUG-05|2005-08-17 00:00:00|    MK_REP|  6000|           20|2005-08-17 00:00:00|          8|      2005|
|        203|     Susan|07-JUN-02|2002-06-07 00:00:00|    HR_REP|  6500|           40|2002-06-07 00:00:00|          6|      2002|
|        204|   Hermann|07-JUN-02|2002-06-07 00:00:00|    PR_REP| 10000|           70|2002-06-07 00:00:00|          6|      2002|
|        205|   Shelley|07-JUN-02|2002-06-07 00:00:00|    AC_MGR| 12008|          110|2002-06-07 00:00:00|          6|      2002|
|        206|   William|07-JUN-02|2002-06-07 00:00:00|AC_ACCOUNT|  8300|          110|2002-06-07 00:00:00|          6|      2002|
|        100|    Steven|17-JUN-03|2003-06-17 00:00:00|   AD_PRES| 24000|           90|2003-06-17 00:00:00|          6|      2003|
|        101|     Neena|21-SEP-05|2005-09-21 00:00:00|     AD_VP| 17000|           90|2005-09-21 00:00:00|          9|      2005|
|        102|       Lex|13-JAN-01|2001-01-13 00:00:00|     AD_VP| 17000|           90|2001-01-13 00:00:00|          1|      2001|
|        103| Alexander|03-JAN-06|2006-01-03 00:00:00|   IT_PROG|  9000|           60|2006-01-03 00:00:00|          1|      2006|
|        104|     Bruce|21-MAY-07|2007-05-21 00:00:00|   IT_PROG|  6000|           60|2007-05-21 00:00:00|          5|      2007|
|        105|     David|25-JUN-05|2005-06-25 00:00:00|   IT_PROG|  4800|           60|2005-06-25 00:00:00|          6|      2005|
|        106|     Valli|05-FEB-06|2006-02-05 00:00:00|   IT_PROG|  4800|           60|2006-02-05 00:00:00|          2|      2006|
|        107|     Diana|07-FEB-07|2007-02-07 00:00:00|   IT_PROG|  4200|           60|2007-02-07 00:00:00|          2|      2007|
|        108|     Nancy|17-AUG-02|2002-08-17 00:00:00|    FI_MGR| 12008|          100|2002-08-17 00:00:00|          8|      2002|
|        109|    Daniel|16-AUG-02|2002-08-16 00:00:00|FI_ACCOUNT|  9000|          100|2002-08-16 00:00:00|          8|      2002|
|        110|      John|28-SEP-05|2005-09-28 00:00:00|FI_ACCOUNT|  8200|          100|2005-09-28 00:00:00|          9|      2005|
+-----------+----------+---------+-------------------+----------+------+-------------+-------------------+-----------+----------+


dfemp_dep = dfemp.alias("dfe").join(dfdep.alias("dfd"),col("dfe.DEPARTMENT_ID")==col("dfd.DEPARTMENT_ID"),"right").select(col("dfe.DEPARTMENT_ID").alias("emp_mein_depart"),col("dfd.DEPARTMENT_ID").alias("dep_mein_depart"),col("dfe.FIRST_NAME")).show()
+---------------+---------------+----------+
|emp_mein_depart|dep_mein_depart|FIRST_NAME|
+---------------+---------------+----------+
|             10|             10|  Jennifer|
|             20|             20|       Pat|
|             20|             20|   Michael|
|             30|             30|     Karen|
|             30|             30|       Guy|
|             30|             30|     Sigal|
|             30|             30|    Shelli|
|             30|             30| Alexander|
|             30|             30|       Den|
|             40|             40|     Susan|
|             50|             50|    Joshua|
|             50|             50|      John|
|             50|             50|   Stephen|
|             50|             50|    Renske|
|             50|             50|     Hazel|
|             50|             50|        Ki|
|             50|             50|   Michael|
|             50|             50|     Jason|
|             50|             50|        TJ|
|             50|             50|     James|
+---------------+---------------+----------+

>>> dfemp_dep = dfemp.alias("dfe").join(dfdep.alias("dfd"),col("dfe.DEPARTMENT_ID")==col("dfd.DEPARTMENT_ID"),"right").select(col("dfe.DEPARTMENT_ID").alias("emp_mein_depart"),col("dfd.DEPARTMENT_ID").alias("dep_mein_depart"),col("dfd.DEPARTMENT_NAME"),col("dfe.FIRST_NAME")).where(col("emp_mein_depart")=='null').show()
+---------------+---------------+---------------+----------+
|emp_mein_depart|dep_mein_depart|DEPARTMENT_NAME|FIRST_NAME|
+---------------+---------------+---------------+----------+
+---------------+---------------+---------------+----------+

>>> dfemp_dep = dfemp.alias("dfe").join(dfdep.alias("dfd"),col("dfe.DEPARTMENT_ID")==col("dfd.DEPARTMENT_ID"),"right").select(col("dfe.DEPARTMENT_ID").alias("emp_mein_depart"),col("dfd.DEPARTMENT_ID").alias("dep_mein_depart"),col("dfd.DEPARTMENT_NAME"),col("dfe.FIRST_NAME")).where(col("emp_mein_depart")=='110').show()
+---------------+---------------+---------------+----------+
|emp_mein_depart|dep_mein_depart|DEPARTMENT_NAME|FIRST_NAME|
+---------------+---------------+---------------+----------+
|            110|            110|     Accounting|   Shelley|
|            110|            110|     Accounting|   William|
+---------------+---------------+---------------+----------+

>>> dfemp_dep = dfemp.alias("dfe").join(dfdep.alias("dfd"),col("dfe.DEPARTMENT_ID")==col("dfd.DEPARTMENT_ID"),"right").select(col("dfe.DEPARTMENT_ID").alias("emp_mein_depart"),col("dfd.DEPARTMENT_ID").alias("dep_mein_depart"),col("dfd.DEPARTMENT_NAME"),col("dfe.FIRST_NAME")).where(col("emp_mein_depart").isNull()).show()
+---------------+---------------+--------------------+----------+
|emp_mein_depart|dep_mein_depart|     DEPARTMENT_NAME|FIRST_NAME|
+---------------+---------------+--------------------+----------+
|           null|             80|               Sales|      null|
|           null|            120|            Treasury|      null|
|           null|            130|       Corporate Tax|      null|
|           null|            140|  Control And Credit|      null|
|           null|            150|Shareholder Services|      null|
|           null|            160|            Benefits|      null|
|           null|            170|       Manufacturing|      null|
|           null|            180|        Construction|      null|
|           null|            190|         Contracting|      null|
|           null|            200|          Operations|      null|
|           null|            210|          IT Support|      null|
|           null|            220|                 NOC|      null|
|           null|            230|         IT Helpdesk|      null|
|           null|            240|    Government Sales|      null|
|           null|            250|        Retail Sales|      null|
|           null|            260|          Recruiting|      null|
|           null|            270|             Payroll|      null|
+---------------+---------------+--------------------+----------+

dfemp_loc = dfemp.join(dfdep,(dfemp.DEPARTMENT_ID==dfdep.DEPARTMENT_ID) & (dfdep.LOCATION_ID==1700),"inner").select(dfemp.FIRST_NAME,dfemp.DEPARTMENT_ID,dfdep.DEPARTMENT_ID,dfdep.LOCATION_ID).show()
+-----------+-------------+-------------+-----------+
| FIRST_NAME|DEPARTMENT_ID|DEPARTMENT_ID|LOCATION_ID|
+-----------+-------------+-------------+-----------+
|   Jennifer|           10|           10|       1700|
|    Shelley|          110|          110|       1700|
|    William|          110|          110|       1700|
|     Steven|           90|           90|       1700|
|      Neena|           90|           90|       1700|
|        Lex|           90|           90|       1700|
|      Nancy|          100|          100|       1700|
|     Daniel|          100|          100|       1700|
|       John|          100|          100|       1700|
|     Ismael|          100|          100|       1700|
|Jose Manuel|          100|          100|       1700|
|       Luis|          100|          100|       1700|
|        Den|           30|           30|       1700|
|  Alexander|           30|           30|       1700|
|     Shelli|           30|           30|       1700|
|      Sigal|           30|           30|       1700|
|        Guy|           30|           30|       1700|
|      Karen|           30|           30|       1700|
+-----------+-------------+-------------+-----------+


>>> location = [(1700,"Hyderabad"),(1400,"Sunabeda")]
>>> schema = StructType([StructField("LOCATION_ID",IntegerType(),True),StructField("LOCATION_NAME",StringType(),True)])
>>> dfloc = spark.createDataFrame(data=location,schema=schema)
>>> dfloc.show()
+-----------+-------------+                                                     
|LOCATION_ID|LOCATION_NAME|
+-----------+-------------+
|       1700|    Hyderabad|
|       1400|     Sunabeda|
+-----------+-------------+
>>> dfemp_loc = dfemp.join(dfdep,dfemp.DEPARTMENT_ID==dfdep.DEPARTMENT_ID,"inner").select(dfemp.FIRST_NAME,dfemp.DEPARTMENT_ID,dfdep.DEPARTMENT_ID,dfdep.LOCATION_ID).join(dfloc,dfdep.LOCATION_ID==dfloc.LOCATION_ID,"inner").select(dfemp.FIRST_NAME,dfdep.DEPARTMENT_ID,dfloc.LOCATION_ID).show()
+-----------+-------------+-----------+
| FIRST_NAME|DEPARTMENT_ID|LOCATION_ID|
+-----------+-------------+-----------+
|      Karen|           30|       1700|
|        Guy|           30|       1700|
|      Sigal|           30|       1700|
|     Shelli|           30|       1700|
|  Alexander|           30|       1700|
|        Den|           30|       1700|
|       Luis|          100|       1700|
|Jose Manuel|          100|       1700|
|     Ismael|          100|       1700|
|       John|          100|       1700|
|     Daniel|          100|       1700|
|      Nancy|          100|       1700|
|        Lex|           90|       1700|
|      Neena|           90|       1700|
|     Steven|           90|       1700|
|    William|          110|       1700|
|    Shelley|          110|       1700|
|   Jennifer|           10|       1700|
|      Diana|           60|       1400|
|      Valli|           60|       1400|
+-----------+-------------+-----------+

>>> def firstLower(in_str):
...     out_str = in_str[0].lower() + in_str[1:]
...     return out_str
... 
>>> print(firstLower("Hhello"))
hhello
>>> firstLowerUDF = udf(lambda i:firstLower(i),StringType())
>>> dfemp.select(col("FIRST_NAME"),col("LAST_NAME")).withColumn("UFirstname",firstLowerUDF(col("FIRST_NAME"))).withColumn("Ulastname",firstLowerUDF(col("LAST_NAME"))).show()
+----------+---------+----------+---------+
|FIRST_NAME|LAST_NAME|UFirstname|Ulastname|
+----------+---------+----------+---------+
|    Donald| OConnell|    donald| oConnell|
|   Douglas|    Grant|   douglas|    grant|
|  Jennifer|   Whalen|  jennifer|   whalen|
|   Michael|Hartstein|   michael|hartstein|
|       Pat|      Fay|       pat|      fay|
|     Susan|   Mavris|     susan|   mavris|
|   Hermann|     Baer|   hermann|     baer|
|   Shelley|  Higgins|   shelley|  higgins|
|   William|    Gietz|   william|    gietz|
|    Steven|     King|    steven|     king|
|     Neena|  Kochhar|     neena|  kochhar|
|       Lex|  De Haan|       lex|  de Haan|
| Alexander|   Hunold| alexander|   hunold|
|     Bruce|    Ernst|     bruce|    ernst|
|     David|   Austin|     david|   austin|
|     Valli|Pataballa|     valli|pataballa|
|     Diana|  Lorentz|     diana|  lorentz|
|     Nancy|Greenberg|     nancy|greenberg|
|    Daniel|   Faviet|    daniel|   faviet|
|      John|     Chen|      john|     chen|
+----------+---------+----------+---------+

>>> @udf(returnType=StringType())
... def uppercaseudf(in_str):
...     out_str = in_str.upper()
...     return out_str
... 
>>> dfemp.select(col("FIRST_NAME"),col("LAST_NAME")).withColumn("UFirstname",uppercaseudf(col("FIRST_NAME"))).withColumn("Ulastname",uppercaseudf(col("LAST_NAME"))).show()
+----------+---------+----------+---------+
|FIRST_NAME|LAST_NAME|UFirstname|Ulastname|
+----------+---------+----------+---------+
|    Donald| OConnell|    DONALD| OCONNELL|
|   Douglas|    Grant|   DOUGLAS|    GRANT|
|  Jennifer|   Whalen|  JENNIFER|   WHALEN|
|   Michael|Hartstein|   MICHAEL|HARTSTEIN|
|       Pat|      Fay|       PAT|      FAY|
|     Susan|   Mavris|     SUSAN|   MAVRIS|
|   Hermann|     Baer|   HERMANN|     BAER|
|   Shelley|  Higgins|   SHELLEY|  HIGGINS|
|   William|    Gietz|   WILLIAM|    GIETZ|
|    Steven|     King|    STEVEN|     KING|
|     Neena|  Kochhar|     NEENA|  KOCHHAR|
|       Lex|  De Haan|       LEX|  DE HAAN|
| Alexander|   Hunold| ALEXANDER|   HUNOLD|
|     Bruce|    Ernst|     BRUCE|    ERNST|
|     David|   Austin|     DAVID|   AUSTIN|
|     Valli|Pataballa|     VALLI|PATABALLA|
|     Diana|  Lorentz|     DIANA|  LORENTZ|
|     Nancy|Greenberg|     NANCY|GREENBERG|
|    Daniel|   Faviet|    DANIEL|   FAVIET|
|      John|     Chen|      JOHN|     CHEN|
+----------+---------+----------+---------+


>>> dfrank.select("*").where(dfrank.rank_salary == '1').join(dfdep,dfemp.DEPARTMENT_ID==dfdep.DEPARTMENT_ID,"inner").select(dfdep.DEPARTMENT_NAME,dfdep.DEPARTMENT_ID).show()
+----------------+-------------+
| DEPARTMENT_NAME|DEPARTMENT_ID|
+----------------+-------------+
|  Administration|           10|
|         Finance|          100|
|      Accounting|          110|
|       Marketing|           20|
|      Purchasing|           30|
| Human Resources|           40|
|        Shipping|           50|
|              IT|           60|
|Public Relations|           70|
|       Executive|           90|
|       Executive|           90|
+----------------+-------------+

>>> dfrank.select("*").where(dfrank.rank_salary == '1').join(dfdep,dfrank.DEPARTMENT_ID==dfdep.DEPARTMENT_ID,"inner").select(dfdep.DEPARTMENT_NAME,dfdep.DEPARTMENT_ID,dfrank.rank_salary).show()
+----------------+-------------+-----------+
| DEPARTMENT_NAME|DEPARTMENT_ID|rank_salary|
+----------------+-------------+-----------+
|  Administration|           10|          1|
|         Finance|          100|          1|
|      Accounting|          110|          1|
|       Marketing|           20|          1|
|      Purchasing|           30|          1|
| Human Resources|           40|          1|
|        Shipping|           50|          1|
|              IT|           60|          1|
|Public Relations|           70|          1|
|       Executive|           90|          1|
|       Executive|           90|          1|
+----------------+-------------+-----------+


>>> dfemp1 = dfemp.withColumn("NEW_SALARY",col("SALARY").cast(IntegerType()))
>>> dfemp1
DataFrame[EMPLOYEE_ID: string, FIRST_NAME: string, LAST_NAME: string, EMAIL: string, PHONE_NUMBER: string, HIRE_DATE: string, JOB_ID: string, SALARY: string, COMMISSION_PCT: string, MANAGER_ID: string, DEPARTMENT_ID: string, NEW_SALARY: int]
>>> WindowSpec = Window.partitionBy("DEPARTMENT_ID").orderBy(col("NEW_SALARY").desc())
>>> dfrank = dfemp1.withColumn("highest_salary_rank", rank().over(Window.partitionBy("DEPARTMENT_ID").orderBy(col("NEW_SALARY").desc()))).select("DEPARTMENT_ID","SALARY","highest_salary_rank").show(100);
+-------------+------+-------------------+
|DEPARTMENT_ID|SALARY|highest_salary_rank|
+-------------+------+-------------------+
|           10|  4400|                  1|
|          100| 12008|                  1|
|          100|  9000|                  2|
|          100|  8200|                  3|
|          100|  7800|                  4|
|          100|  7700|                  5|
|          100|  6900|                  6|
|          110| 12008|                  1|
|          110|  8300|                  2|
|           20| 13000|                  1|
|           20|  6000|                  2|
|           30| 11000|                  1|
|           30|  3100|                  2|
|           30|  2900|                  3|
|           30|  2800|                  4|
|           30|  2600|                  5|
|           30|  2500|                  6|
+-------------+------+-------------------+

>>> dfrank = dfemp1.withColumn("RUNNING_SUM", sum("SALARY").over(WindowSpec)).select("DEPARTMENT_ID","SALARY","RUNNING_SUM").show();
+-------------+------+-----------+
|DEPARTMENT_ID|SALARY|RUNNING_SUM|
+-------------+------+-----------+
|           10|  4400|     4400.0|
|          100| 12008|    12008.0|
|          100|  9000|    21008.0|
|          100|  8200|    29208.0|
|          100|  7800|    37008.0|
|          100|  7700|    44708.0|
|          100|  6900|    51608.0|
|          110| 12008|    12008.0|
|          110|  8300|    20308.0|
|           20| 13000|    13000.0|
|           20|  6000|    19000.0|
|           30| 11000|    11000.0|
|           30|  3100|    14100.0|
|           30|  2900|    17000.0|
|           30|  2800|    19800.0|
|           30|  2600|    22400.0|
|           30|  2500|    24900.0|
|           40|  6500|     6500.0|
|           50|  8200|     8200.0|
|           50|  8000|    16200.0|
+-------------+------+-----------+


>>> jsonDf = spark.read.json('/input-data/jsonexample.json')
>>> jsonDf.select(jsonDf.Text1, jsonDf.Array1[2]).show()
+-----+---------+
|Text1|Array1[2]|
+-----+---------+
|Hello|        9|
| This|       91|
|  Yes|        3|
+-----+---------+

>>> jsonDf.select(jsonDf.Text1, explode(jsonDf.Array1)).show()
+-----+---+
|Text1|col|
+-----+---+
|Hello|  7|
|Hello|  8|
|Hello|  9|
| This| 70|
| This| 88|
| This| 91|
|  Yes|  1|
|  Yes|  2|
|  Yes|  3|
+-----+---+

>>> dataDictionary = [
...         ('James',{'hair':'black','eye':'brown'}),
...         ('Michael',{'hair':'brown','eye':None}),
...         ('Robert',{'hair':'red','eye':'black'}),
...         ('Washington',{'hair':'grey','eye':'grey'}),
...         ('Jefferson',{'hair':'brown','eye':''})
...         ]
schema = StructType([StructField("Name",StringType(),False),StructField("Properties",MapType(StringType(),StringType(),False))])

>>> schema = StructType([StructField("Name",StringType(),False),StructField("Properties",MapType(StringType(),StringType(),True))])
>>> dfmap = spark.createDataFrame(data=dataDictionary,schema=schema)
>>> dfmap.show()
+----------+--------------------+                                               
|      Name|          Properties|
+----------+--------------------+
|     James|{eye -> brown, ha...|
|   Michael|{eye -> null, hai...|
|    Robert|{eye -> black, ha...|
|Washington|{eye -> grey, hai...|
| Jefferson|{eye -> , hair ->...|
+----------+--------------------+

>>> dfmap.withColumn("hair",dfmap.Properties["hair"]).withColumn("eye",dfmap.Properties["eye"]).show()
+----------+--------------------+-----+-----+
|      Name|          Properties| hair|  eye|
+----------+--------------------+-----+-----+
|     James|{eye -> brown, ha...|black|brown|
|   Michael|{eye -> null, hai...|brown| null|
|    Robert|{eye -> black, ha...|  red|black|
|Washington|{eye -> grey, hai...| grey| grey|
| Jefferson|{eye -> , hair ->...|brown|     |
+----------+--------------------+-----+-----+


>>> dfmap.select(dfmap.Name,explode(dfmap.Properties)).show()
+----------+----+-----+
|      Name| key|value|
+----------+----+-----+
|     James| eye|brown|
|     James|hair|black|
|   Michael| eye| null|
|   Michael|hair|brown|
|    Robert| eye|black|
|    Robert|hair|  red|
|Washington| eye| grey|
|Washington|hair| grey|
| Jefferson| eye|     |
| Jefferson|hair|brown|
+----------+----+-----+

>>> dfrep = dfexp.withColumn("New_Name",regexp_replace("Name","es","ie"))
>>> dfrep.show()
+----------+----+-----+----------+
|      Name| key|value|  New_Name|
+----------+----+-----+----------+
|     James| eye|brown|     Jamie|
|     James|hair|black|     Jamie|
|   Michael| eye| null|   Michael|
|   Michael|hair|brown|   Michael|
|    Robert| eye|black|    Robert|
|    Robert|hair|  red|    Robert|
|Washington| eye| grey|Washington|
|Washington|hair| grey|Washington|
| Jefferson| eye|     | Jefferson|
| Jefferson|hair|brown| Jefferson|
+----------+----+-----+----------+





   
   
   








































































